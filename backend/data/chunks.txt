4/14/2025 1 CS4051- Information Retrieval Muhammad Rafi March 25, 2025 Language Model for IR Week 12 4/14/2025 2 Agenda IR Approaches (IR models) Language Model –Basic Examples Types of Language Model Maximum Likelihood Estimates Extended Language Models Vector Space Vs. Language Model Conclusion IR Approaches Boolean model Based on the notion of sets Documents are retrieved only if they satisfy Boolean conditions specified in the query Does not impose a ranking on retrieved documents Exact match Vector space model Based on geometry, the notion of vectors in high dimensional space Documents are ranked based on their similarity to the query (ranked retrieval) Best/partial match 4/14/2025 3 IR Approaches Probabilistic models Based on the notion of probabilities present in the text Document are ranked based on the probabilities that the text present in document based on query terms.
3 IR Approaches Probabilistic models Based on the notion of probabilities present in the text Document are ranked based on the probabilities that the text present in document based on query terms. Best/partial match Language models Based on the notion of probabilities and processes for generating text Documents are ranked based on the probability that they generated the query Best/partial match Language Model –Basic Idea A common suggestion to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as the query. The language modeling approach to IR directly models that idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often.
to IR directly models that idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often. 4/14/2025 4 LM vs. PM Instead of overtly modeling the probability P(R = 1|q, d) of relevance of a document d to a query q, as in the traditional probabilistic approach to IR Language modeling approach instead builds a probabilistic language model Md from each document d, and ranks documents based on the probability of the model generating the query: P(q|Md). Language Model A language model is a function that puts a probability measure over strings drawn from some vocabulary.
s documents based on the probability of the model generating the query: P(q|Md). Language Model A language model is a function that puts a probability measure over strings drawn from some vocabulary. That is, for a language model M over an alphabet S: One simple kind of language model is equivalent to a probabilistic finite automaton consisting of just a single node with a single probability distribution over producing different terms, so that 4/14/2025 5 Example 1 Example 1 4/14/2025 6 Example 2 Example 2 4/14/2025 7 Language Model  Each document is treated as (the basis for) a language model.  Given a query q  Rank documents based on P(d|q)  P(q) is the same for all documents, so ignore  P(d) is the prior – often treated as the same for all d  But we can give a prior to “high-quality” documents, e.g., those with high PageRank.  P(q|d) is the probability of q given d.  So to rank documents according to relevance to q, ranking according to P(q|d) and P(d|q) is equivalent.
-quality” documents, e.g., those with high PageRank.  P(q|d) is the probability of q given d.  So to rank documents according to relevance to q, ranking according to P(q|d) and P(d|q) is equivalent. Language Model p(d) = prior probability of hypothesis A – PRIOR p(q) = prior probability of query q – EVIDENCE p(q|d) = probability of q given d - LIKELIHOOD P(d|q) = probability of d given q – POSTERIOR 4/14/2025 8 Query Likelihood Model Query Likelihood Model 4/14/2025 9 Query Likelihood Model Query Likelihood Model 4/14/2025 10 Query Likelihood Model Query Likelihood Model 4/14/2025 11 Query Likelihood Model Query Likelihood Model 4/14/2025 12 Query Likelihood Model Example 4/14/2025 13 Smoothing- Language Model Jelinek-Mercer smoothing Dirichlet smoothing Good-Turing smoothing Extended Language Model There are other ways to think of using the language modeling idea in IR settings, and many of them have been tried in subsequent work.
richlet smoothing Good-Turing smoothing Extended Language Model There are other ways to think of using the language modeling idea in IR settings, and many of them have been tried in subsequent work. Rather than looking at the probability of a document language model Md generating the query, you can look at the probability of a query language model Mq generating the document. The main reason that doing things in this direction and creating a document likelihood model is less appealing is that there is much less text available to estimate a language model based on the query text, and so the model will be worse estimated. (Smoothing factors) 4/14/2025 14 Extended Language Model Kullback Leibler Divergence Model 4/14/2025 15 KL Divergence Model LM vs. VSM LMs have some things in common with vector space models. Term frequency is directed in the model. But it is not scaled in LMs. Probabilities are inherently “length-normalized”.
ce Model LM vs. VSM LMs have some things in common with vector space models. Term frequency is directed in the model. But it is not scaled in LMs. Probabilities are inherently “length-normalized”. Cosine normalization does something similar for vector space. Mixing document and collection frequencies has an effect similar to idf. Terms rare in the general collection, but common in some documents will have a greater influence on the ranking. 4/14/2025 16 LM vs. VSM LMs vs. vector space model: commonalities Term frequency is directly in the model. Probabilities are inherently “length-normalized”. Mixing document and collection frequencies has an effect similar to idf. LMs vs. vector space model: differences LMs: based on probability theory Vector space: based on similarity, a geometric/ linear algebra notion Collection frequency vs. document frequency Details of term frequency, length normalization etc.
s: based on probability theory Vector space: based on similarity, a geometric/ linear algebra notion Collection frequency vs. document frequency Details of term frequency, length normalization etc. Assumptions of LM Simplifying assumption: Queries and documents are objects of same type. Not true! There are other LMs for IR that do not make this assumption. The vector space model makes the same assumption. Simplifying assumption: Terms are conditionally independent. Again, vector space model (and Naive Bayes) makes the same assumption. Cleaner statement of assumptions than vector space Thus, better theoretical foundation than vector space … but “pure” LMs perform much worse than “tuned” LMs.
4/27/2025 1 CS4051 Information Retrieval Week 13 Muhammad Rafi April 14, 2025 Web Search Basics & Web Crawling Chapter No. 19 and 20 4/27/2025 2 Today’s Agenda Web Search basic Background & History Web Characteristics The search user experience Economic Models Index Estimates Duplicate detection Web Crawler Conclusion Web Search Basics 4/27/2025 3 Web Search Basics Internet is a Client Server Architecture provides a bunch of services. Client The client – generally a browser, an application within a graphical user environment Server The server communicates with the client via a protocol HTTP It is lightweight and simple, asynchronously carrying a variety of payloads (text, images and – over time – richer media such as audio and video files) encoded in a simple markup language called HTML (for hypertext markup language) HTML – It is a markup language for the web.
(text, images and – over time – richer media such as audio and video files) encoded in a simple markup language called HTML (for hypertext markup language) HTML – It is a markup language for the web. Connect different pages and content Web Search – Client Server Browser The first browser was developed by Tim Berners- Lee in 1990- very limited functionality Mosaic was first GUI based browser in 1993 by Marc Andreesen Marc started Netscape in 1994 and launch Netscape Navigator Microsoft started IE in 1995 for free. 95% market share in 2002 Marc started Mozilla foundation and started Firefox in 2004 reached 23% market share in 2011 4/27/2025 4 Web Search – Client Server HTTP HTTP is an application protocol for distributed, collaborative, and hypermedia information systems. HTTP/2, was standardized in 2015, and is now supported by major web servers and browsers. HTTP Header contains a lot of fields for effective transfer of information.
hypermedia information systems. HTTP/2, was standardized in 2015, and is now supported by major web servers and browsers. HTTP Header contains a lot of fields for effective transfer of information. Web Search – Client Server 4/27/2025 5 Web Search – Client Server HTTP There are five groups of status codes which are grouped by the first digit: 1xx — Informational. 2xx — The request was successful. 3xx — The client is redirected to a different resource. 4xx — The request contains an error of some kind. 5xx — The server encountered an error fulfilling the request. Web Search – Client Server HTTPS The secure version of HTTP protocol is HyperText Transfer Protocol Secure. In HTTPS, the communication protocol is encrypted using Transport Layer Security (TLS) or Secure Sockets Layer (SSL) Benefits of HTTPS  Customer information, like credit card numbers and other sensitive information, is encrypted and cannot be intercepted.
ransport Layer Security (TLS) or Secure Sockets Layer (SSL) Benefits of HTTPS  Customer information, like credit card numbers and other sensitive information, is encrypted and cannot be intercepted.  Visitors can verify you are a registered business and that you own the domain.  Customers know they are not suppose to visit sites without HTTPS, and therefore, they are more likely to trust and complete purchases from sites that use HTTPS. 4/27/2025 6 Web Search – Client Server HTML HTML 2.0 -1995; HTML 3.0 1997; HTML 4.0 1997 HTML 5.0 2014; XHTML vs. XML Server Side Scripting A number of server side scripting available. Client Side Scripting Generally UI and interaction with local machine, mostly Java Script Cascading Style Sheet (CSS) CSS is a language that describes the style of an HTML document. Web Search – Client Server HTTP Injection HTML Injection also known as Cross Site Scripting.
Java Script Cascading Style Sheet (CSS) CSS is a language that describes the style of an HTML document. Web Search – Client Server HTTP Injection HTML Injection also known as Cross Site Scripting. It is a security vulnerability that allows an attacker to inject HTML code into web pages that are viewed by other users. HTTP Response Splitting Web Application Vulnerability Web Cache poisoning Cross-User Defacement HTTP Cross Site Scripting Session Fixation 4/27/2025 7 Client-Side Vs. Server Side Scripting Web Information Discovery Directories Taxonomies populated with web pages in categories, such as Yahoo! The user to browse through a hierarchical tree of category labels. Search Engines Full-text index search engines such as Altavista, Excite and Infoseek The user with a keyword search interface supported by inverted indexes and ranking mechanisms.
category labels. Search Engines Full-text index search engines such as Altavista, Excite and Infoseek The user with a keyword search interface supported by inverted indexes and ranking mechanisms. 4/27/2025 8 Web Information Discovery Directories Vs. Search Engines A directory allows you to explore and get what you want eventually. Use a directory to find cooking-related websites. Use a directory to find travel guides in a country. A search engine brings you to the exact page on the words or phrases you are looking for. Use a search engine to find a specific recipe, by providing the name of the ingredients.
uides in a country. A search engine brings you to the exact page on the words or phrases you are looking for. Use a search engine to find a specific recipe, by providing the name of the ingredients. Use a search engine to find the transport trains schedule in Germany 4/27/2025 9 Web Search The Web Ad indexes Web spider Indexer Indexes Search User Web Characteristics Web User Interaction Web as a Graph Web Spam 4/27/2025 10 Web Characteristics Web as a Graph There are three major categories of web pages that are IN, OUT and SCC Web Economic Model Advertisement Model for Revenue Unit of Measurement CPM, CPC, CPI, CPD, CPP Complex Advertisement Models AdWords Ads Search terms Daily budget 4/27/2025 11 AdWords User Needs  Need [Brod02, RL04] Informational – want to learn about something (~40% / 65%) Navigational – want to go to that page (~25% / 15%) Transactional – want to do something (web-mediated) (~35% / 20%)  Access a service  Downloads  Shop Gray areas  Find a good hub  Exploratory search “see what’s there” Low hemoglobin United Airlines Seattle weather Mars surface images Canon S410 Car rental Brazil Sec.
a service  Downloads  Shop Gray areas  Find a good hub  Exploratory search “see what’s there” Low hemoglobin United Airlines Seattle weather Mars surface images Canon S410 Car rental Brazil Sec. 19.4.1 4/27/2025 12 How far do people look for results? (Source: iprospect.com WhitePaper_2006_SearchEngineUserBehavior.pdf) User Experience User Queries 3-4 Keywords Seldom uses syntax operators (Free Text Queries) Search Engines: Google identified two principles that helped it grow at the expense of its competitors Relevance Simple Interface Which Search engine is Bigger? 4/27/2025 13 Index Size & Estimate Capture / Recapture Method Suppose that we could pick a random page from the index of E1 and test whether it is in E2’s index and symmetrically, test whether a random page from E2 is in E1. These experiments give us fractions x and y such that our estimate is that a fraction x of the pages in E1 are in E2, while a fraction y of the pages in E2 are in E1.
a random page from E2 is in E1. These experiments give us fractions x and y such that our estimate is that a fraction x of the pages in E1 are in E2, while a fraction y of the pages in E2 are in E1. Then, letting |Ei| denote the size of the index of search engine Ei, we have x|E1| ≈ y|E2|,from which we have the form we will use |E1|/|E2| ≈ y/x Index Size & Estimate Sampling Methods Random Searches Random IP addresses Random Walks Random Queries Actual Estimate is quite challenging 4/27/2025 14 Duplicate / Near Duplicate Detection Web pages are mirrored for redundancy and high availability, hence while indexing for web search engine we may come up for duplicate (identical copy). Checksum is a common method to detect a duplicate. Near Duplicate – not identical, but a portion is common, based on pre-set threshold we can filter out the near duplicates.
plicate (identical copy). Checksum is a common method to detect a duplicate. Near Duplicate – not identical, but a portion is common, based on pre-set threshold we can filter out the near duplicates. Shingling - Given a positive integer k and a sequence of terms in a document d, define the k- shingles of d to be the set of all consecutive sequences of k terms in d. Shingling To find a near duplicate, a shingling approach is used. If there are many common shingling for some k in a pair of documents, its contents will be the same. Consider a sentence below a rose is a rose is a rose. Its shingling set Z = {a rose is a ; rose is a rose ; is a rose is ; a rose is a ; rose is a rose } , which has |Z|=5 Overlap, by Jaccard = 2/5 4/27/2025 15 Near-Duplicate Scaled Approach A pair-wise approach seems unavoidable for using shingling overlap to detect near duplicate. We can perform better, by using a large integer Hash Function and doing Hashing for shingling patterns.
h A pair-wise approach seems unavoidable for using shingling overlap to detect near duplicate. We can perform better, by using a large integer Hash Function and doing Hashing for shingling patterns. Near-Duplicate Scaled Approach 4/27/2025 16 Web Crawler Web crawling is the process by which we gather pages from the Web to index them and support a search engine. The objective of crawling is to quickly and efficiently gather as many useful web pages as possible, together with the link structure that interconnects them. web crawler is sometimes referred to as a spider. Feature a Crawler MUST provide Robustness: The crawler must be robust to deal with a large number of linked pages from a website. Sometime server traps a crawler, the crawler must identify these traps. Politeness: Web servers have both implicit and explicit policies regulating the rate at which a crawler can visit them. These politeness policies must be respected.
crawler must identify these traps. Politeness: Web servers have both implicit and explicit policies regulating the rate at which a crawler can visit them. These politeness policies must be respected. 4/27/2025 17 Feature a Crawler Should provide Distributed: The crawler should have the ability to execute in a distributed fashion across multiple machines. Scalable: The crawler architecture should permit scaling up the crawl rate by adding extra machines and bandwidth. Performance and efficiency: The crawl system should make efficient use of various system resources including processor, storage, and network bandwidth. Feature a Crawler Should provide Quality: Given that a significant fraction of all web pages are of poor utility for serving user query needs, the crawler should be biased toward fetching “useful” pages first. Freshness: In many applications, the crawler should operate in continuous mode: It should obtain fresh copies of previously fetched pages.
awler should be biased toward fetching “useful” pages first. Freshness: In many applications, the crawler should operate in continuous mode: It should obtain fresh copies of previously fetched pages. 4/27/2025 18 Feature a Crawler Should provide Extensible: Crawlers should be designed to be extensible in many ways – to cope with new data formats, new fetch protocols, and so on. This demands that the crawler architecture be modular. Architecture of a Crawler 4/27/2025 19 Architecture of a Crawler URL Frontier: containing URLs yet to be fetches in the current crawl. At first, a seed set is stored in URL Frontier, and a crawler begins by taking a URL from the seed set. DNS: domain name service resolution. Look up IP address for domain names. Fetch: generally use the http protocol to fetch the URL. Parse: the page is parsed. Texts (images, videos, and etc.) and Links are extracted.
service resolution. Look up IP address for domain names. Fetch: generally use the http protocol to fetch the URL. Parse: the page is parsed. Texts (images, videos, and etc.) and Links are extracted. URL frontier 4/27/2025 20 Typical Crawler Architecture of a Crawler Distributed Indexes By term (global Indexes) By document (Local Indexes) Connectivity Server URL are transformed into Integers values In-Link and Out-Link states are maintained. Ordering of URL based on Host, lexicographic ordering, etc
4/27/2025 1 CS4051 Information Retrieval Week 13 Muhammad Rafi April 22, 2025 Link Analysis Chapter No. 21 4/27/2025 2 Today’s Agenda Link Analysis Web as a Graph Page Rank & Markov Chain Page Rank Computation HITS Algorithm HITS vs. PageRank Conclusion Link Analysis The analysis of hyperlinks and the graph structure of the Web has been instrumental in the development of web search. In this chapter we focus on the use of hyperlinks for ranking web search results. Such link analysis is one of many factors considered by web search engines in computing a composite score for a web page on any given query. Web Page – <In-Link> and <Out-Link> <a href="http://www.acm.org/jacm/">Journal of the ACM.</a> Anchor Text Extended Anchor Text 4/27/2025 3 The Web as a graph Web pages are connected with in- and out- links Our study of link analysis builds on two intuitions: The anchor text pointing to page B is a good description of page B.
27/2025 3 The Web as a graph Web pages are connected with in- and out- links Our study of link analysis builds on two intuitions: The anchor text pointing to page B is a good description of page B. The hyperlink from A to B represents an endorsement of page B, by the creator of page A. The Web as a graph Assumption 1 : A hyperlink between pages denotes a conferral of authority (quality signal) Assumption 2: The text in the anchor of the hyperlink describe the target page( Context/ Textual description of a page). The Web is full of instances where the page B does not provide an accurate description of itself. For example, at the time of the writing of this book the home page of the IBM corporation (http://www.ibm.com)did not contain the term computer anywhere in its HTML code, despite the fact that IBM is widely viewed as the world’s largest computer maker. Thus, there is often a gap between the terms in a web page, and how web users would describe that web page.
TML code, despite the fact that IBM is widely viewed as the world’s largest computer maker. Thus, there is often a gap between the terms in a web page, and how web users would describe that web page. 4/27/2025 4 Page Rank Our first technique for link analysis assigns to every node in the web graph a numerical score between 0 and 1, known as its PageRank. The PageRank of a node will depend on the link structure of the web graph. Given a query, a web search engine computes a composite score for each web page that combines hundreds of features such as cosine similarity and term proximity, together with the PageRank score. Page Rank Consider a random surfer who begins at a web page (a node of the web graph) and executes a random walk on the Web as follows. At each time step, the surfer proceeds from his current page A to a randomly chosen web page that A hyperlinks to.
web page (a node of the web graph) and executes a random walk on the Web as follows. At each time step, the surfer proceeds from his current page A to a randomly chosen web page that A hyperlinks to. As the surfer proceeds in this random walk from node to node, he visits some nodes more often than others; intuitively, these are nodes with many links coming in from other frequently visited nodes. In the teleport operation the surfer jumps from a node to any other node in the web graph. This could happen because he types an address into the URL bar of his browser. Teleporting is uniformly performed. 4/27/2025 5 Teleporting In the teleport operation the surfer jumps from a node to any other node in the web graph. In assigning a PageRank score to each node of the web graph, we use the teleport operation in two ways: When at a node with no out-links, the surfer invokes the teleport operation.
in the web graph. In assigning a PageRank score to each node of the web graph, we use the teleport operation in two ways: When at a node with no out-links, the surfer invokes the teleport operation. At any node that has outgoing links, the surfer invokes the teleport operation with probability 0 < α < 1 and the standard random walk (follow an out-link chosen uniformly at random with probability 1 − α, where α is a fixed parameter chosen in advance. Typically, α might be 0.1. Markov Chain A Markov chain is a discrete-time stochastic process: a process that occurs in a series of time-steps in each of which a random choice is made. A Markov chain consists of N states. Each web page will correspond to a state in the Markov chain we will formulate. A Markov chain is characterized by an N × N transition probability matrix P each of whose entries is in the interval [0, 1]; the entries in each row of P add up to 1.
Markov chain we will formulate. A Markov chain is characterized by an N × N transition probability matrix P each of whose entries is in the interval [0, 1]; the entries in each row of P add up to 1. 4/27/2025 6 Markov Chain The Markov chain can be in one of the N states at any given timestep; then, the entry Pij tells us the probability that the state at the next timestep is j, conditioned on the current state being i. Each entry Pij is known as a transition probability and depends only on the current state i; this is known as the Markov property. Markov Chain as Stochastic Matrix A matrix with non-negative entries that satisfies is known as a stochastic matrix. A key property of a stochastic matrix is that it has a principal left eigenvector corresponding to its largest eigenvalue, which is 1.
negative entries that satisfies is known as a stochastic matrix. A key property of a stochastic matrix is that it has a principal left eigenvector corresponding to its largest eigenvalue, which is 1. An N-dimensional probability vector each of whose components corresponds to one of the N states of a Markov chain can be viewed as a probability distribution over its states Markov Properties 4/27/2025 7 Markov Chain Probability Matrix How to get the probability matrix? If a row of A has no 1’s, then replace each element by 1/N. For all other rows proceed as follows: Divide each 1 in A by the number of 1’s in its row. Thus, if there is a row with three m’s, then each of them is replaced by 1/m. Multiply the resulting matrix by 1 − α.
her rows proceed as follows: Divide each 1 in A by the number of 1’s in its row. Thus, if there is a row with three m’s, then each of them is replaced by 1/m. Multiply the resulting matrix by 1 − α. Add α/N to every entry of the resulting matrix, to obtain the required matrix P. Ergodic Markov chain A Markov chain is said to be ergodic if there exists a positive integer T0 such that for all pairs of states i, j in the Markov chain, if it is started at time 0 in state i then for all t > T0, the probability of being in state j at time t is greater than 0. The random walk with teleporting results in a unique distribution of steady-state probabilities over the states of the induced Markov chain. This steady-state probability for a state is the PageRank of the corresponding web page.
ng results in a unique distribution of steady-state probabilities over the states of the induced Markov chain. This steady-state probability for a state is the PageRank of the corresponding web page. 4/27/2025 8 The PageRank computation The N entries in the principal eigenvector ~π are the steady-state probabilities of the random walk with teleporting, and thus the PageRank values for the corresponding web pages. The PageRank computation 4/27/2025 9 A small Web Example Consider a web graph with three nodes 1, 2 and 3. The links are as follows: 1 → 2, 3 → 2, 2 → 1, 2 → 3. Write down the transition probability matrices for the surfer’s walk with teleporting, for the following three values of the teleport probability: (a) α = 0; (b) α = 0.5 and (c) α = 1. Advantage/Disadvantage PageRank 4/27/2025 10 HITS Hyperlink-Induced Topic Search (HITS; also known as hubs and authorities) is a link analysis algorithm that rates Web pages, developed by Jon Kleinberg.
antage/Disadvantage PageRank 4/27/2025 10 HITS Hyperlink-Induced Topic Search (HITS; also known as hubs and authorities) is a link analysis algorithm that rates Web pages, developed by Jon Kleinberg. Hubs, served as large directories that were not actually authoritative in the information that it held, but were used as compilations of a broad catalog of information that led users directly to other authoritative pages. HITS In other words, a good hub represented a page that pointed to many other pages, and a good authority represented a page that was linked by many different hubs The algorithm assigns two scores for each page: its authority, which estimates the value of the content of the page, and its hub value, which estimates the value of its links to other pages.
ent hubs The algorithm assigns two scores for each page: its authority, which estimates the value of the content of the page, and its hub value, which estimates the value of its links to other pages. 4/27/2025 11 Example HITS Algorithm 4/27/2025 12 HITS Algorithm HITS Algorithm 4/27/2025 13 HITS Algorithm An Example (HITS) 4/27/2025 14 HITS-Issues  It is query dependent, that is, the (Hubs and Authority) scores resulting from the link analysis are influenced by the search terms;  As a corollary, it is executed at query time, not at indexing time, with the associated hit on performance that accompanies query-time processing.  It is not commonly used by search engines. (Though a similar algorithm was said to be used by Teoma, which was acquired by Ask Jeeves/Ask.com.)
on performance that accompanies query-time processing.  It is not commonly used by search engines. (Though a similar algorithm was said to be used by Teoma, which was acquired by Ask Jeeves/Ask.com.)  It computes two scores per document, hub and authority, as opposed to a single score;  It is processed on a small subset of ‘relevant’ documents (a 'focused subgraph' or base set), not all documents as was the case with PageRank. Example: A subset of graph with selected Hub & Authority status. This is a result of resultant search result on “q” 4/27/2025 15 Adjacency Matrix Iterative calculation of Hub & Authority 4/27/2025 16 Iterative calculation of Hub & Authority Normalized 4/27/2025 17 HITS vs. PageRank
4/14/2025 1 CS4051- Information Retrieval Muhammad Rafi March 25, 2025 Language Model for IR Week 12 4/14/2025 2 Agenda IR Approaches (IR models) Language Model –Basic Examples Types of Language Model Maximum Likelihood Estimates Extended Language Models Vector Space Vs. Language Model Conclusion IR Approaches Boolean model Based on the notion of sets Documents are retrieved only if they satisfy Boolean conditions specified in the query Does not impose a ranking on retrieved documents Exact match Vector space model Based on geometry, the notion of vectors in high dimensional space Documents are ranked based on their similarity to the query (ranked retrieval) Best/partial match 4/14/2025 3 IR Approaches Probabilistic models Based on the notion of probabilities present in the text Document are ranked based on the probabilities that the text present in document based on query terms.
3 IR Approaches Probabilistic models Based on the notion of probabilities present in the text Document are ranked based on the probabilities that the text present in document based on query terms. Best/partial match Language models Based on the notion of probabilities and processes for generating text Documents are ranked based on the probability that they generated the query Best/partial match Language Model –Basic Idea A common suggestion to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as the query. The language modeling approach to IR directly models that idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often.
to IR directly models that idea: a document is a good match to a query if the document model is likely to generate the query, which will in turn happen if the document contains the query words often. 4/14/2025 4 LM vs. PM Instead of overtly modeling the probability P(R = 1|q, d) of relevance of a document d to a query q, as in the traditional probabilistic approach to IR Language modeling approach instead builds a probabilistic language model Md from each document d, and ranks documents based on the probability of the model generating the query: P(q|Md). Language Model A language model is a function that puts a probability measure over strings drawn from some vocabulary.
s documents based on the probability of the model generating the query: P(q|Md). Language Model A language model is a function that puts a probability measure over strings drawn from some vocabulary. That is, for a language model M over an alphabet S: One simple kind of language model is equivalent to a probabilistic finite automaton consisting of just a single node with a single probability distribution over producing different terms, so that 4/14/2025 5 Example 1 Example 1 4/14/2025 6 Example 2 Example 2 4/14/2025 7 Language Model  Each document is treated as (the basis for) a language model.  Given a query q  Rank documents based on P(d|q)  P(q) is the same for all documents, so ignore  P(d) is the prior – often treated as the same for all d  But we can give a prior to “high-quality” documents, e.g., those with high PageRank.  P(q|d) is the probability of q given d.  So to rank documents according to relevance to q, ranking according to P(q|d) and P(d|q) is equivalent.
-quality” documents, e.g., those with high PageRank.  P(q|d) is the probability of q given d.  So to rank documents according to relevance to q, ranking according to P(q|d) and P(d|q) is equivalent. Language Model p(d) = prior probability of hypothesis A – PRIOR p(q) = prior probability of query q – EVIDENCE p(q|d) = probability of q given d - LIKELIHOOD P(d|q) = probability of d given q – POSTERIOR 4/14/2025 8 Query Likelihood Model Query Likelihood Model 4/14/2025 9 Query Likelihood Model Query Likelihood Model 4/14/2025 10 Query Likelihood Model Query Likelihood Model 4/14/2025 11 Query Likelihood Model Query Likelihood Model 4/14/2025 12 Query Likelihood Model Example 4/14/2025 13 Smoothing- Language Model Jelinek-Mercer smoothing Dirichlet smoothing Good-Turing smoothing Extended Language Model There are other ways to think of using the language modeling idea in IR settings, and many of them have been tried in subsequent work.
richlet smoothing Good-Turing smoothing Extended Language Model There are other ways to think of using the language modeling idea in IR settings, and many of them have been tried in subsequent work. Rather than looking at the probability of a document language model Md generating the query, you can look at the probability of a query language model Mq generating the document. The main reason that doing things in this direction and creating a document likelihood model is less appealing is that there is much less text available to estimate a language model based on the query text, and so the model will be worse estimated. (Smoothing factors) 4/14/2025 14 Extended Language Model Kullback Leibler Divergence Model 4/14/2025 15 KL Divergence Model LM vs. VSM LMs have some things in common with vector space models. Term frequency is directed in the model. But it is not scaled in LMs. Probabilities are inherently “length-normalized”.
ce Model LM vs. VSM LMs have some things in common with vector space models. Term frequency is directed in the model. But it is not scaled in LMs. Probabilities are inherently “length-normalized”. Cosine normalization does something similar for vector space. Mixing document and collection frequencies has an effect similar to idf. Terms rare in the general collection, but common in some documents will have a greater influence on the ranking. 4/14/2025 16 LM vs. VSM LMs vs. vector space model: commonalities Term frequency is directly in the model. Probabilities are inherently “length-normalized”. Mixing document and collection frequencies has an effect similar to idf. LMs vs. vector space model: differences LMs: based on probability theory Vector space: based on similarity, a geometric/ linear algebra notion Collection frequency vs. document frequency Details of term frequency, length normalization etc.
s: based on probability theory Vector space: based on similarity, a geometric/ linear algebra notion Collection frequency vs. document frequency Details of term frequency, length normalization etc. Assumptions of LM Simplifying assumption: Queries and documents are objects of same type. Not true! There are other LMs for IR that do not make this assumption. The vector space model makes the same assumption. Simplifying assumption: Terms are conditionally independent. Again, vector space model (and Naive Bayes) makes the same assumption. Cleaner statement of assumptions than vector space Thus, better theoretical foundation than vector space … but “pure” LMs perform much worse than “tuned” LMs.
4/27/2025 1 CS4051 Information Retrieval Week 13 Muhammad Rafi April 14, 2025 Web Search Basics & Web Crawling Chapter No. 19 and 20 4/27/2025 2 Today’s Agenda Web Search basic Background & History Web Characteristics The search user experience Economic Models Index Estimates Duplicate detection Web Crawler Conclusion Web Search Basics 4/27/2025 3 Web Search Basics Internet is a Client Server Architecture provides a bunch of services. Client The client – generally a browser, an application within a graphical user environment Server The server communicates with the client via a protocol HTTP It is lightweight and simple, asynchronously carrying a variety of payloads (text, images and – over time – richer media such as audio and video files) encoded in a simple markup language called HTML (for hypertext markup language) HTML – It is a markup language for the web.
(text, images and – over time – richer media such as audio and video files) encoded in a simple markup language called HTML (for hypertext markup language) HTML – It is a markup language for the web. Connect different pages and content Web Search – Client Server Browser The first browser was developed by Tim Berners- Lee in 1990- very limited functionality Mosaic was first GUI based browser in 1993 by Marc Andreesen Marc started Netscape in 1994 and launch Netscape Navigator Microsoft started IE in 1995 for free. 95% market share in 2002 Marc started Mozilla foundation and started Firefox in 2004 reached 23% market share in 2011 4/27/2025 4 Web Search – Client Server HTTP HTTP is an application protocol for distributed, collaborative, and hypermedia information systems. HTTP/2, was standardized in 2015, and is now supported by major web servers and browsers. HTTP Header contains a lot of fields for effective transfer of information.
hypermedia information systems. HTTP/2, was standardized in 2015, and is now supported by major web servers and browsers. HTTP Header contains a lot of fields for effective transfer of information. Web Search – Client Server 4/27/2025 5 Web Search – Client Server HTTP There are five groups of status codes which are grouped by the first digit: 1xx — Informational. 2xx — The request was successful. 3xx — The client is redirected to a different resource. 4xx — The request contains an error of some kind. 5xx — The server encountered an error fulfilling the request. Web Search – Client Server HTTPS The secure version of HTTP protocol is HyperText Transfer Protocol Secure. In HTTPS, the communication protocol is encrypted using Transport Layer Security (TLS) or Secure Sockets Layer (SSL) Benefits of HTTPS  Customer information, like credit card numbers and other sensitive information, is encrypted and cannot be intercepted.
ransport Layer Security (TLS) or Secure Sockets Layer (SSL) Benefits of HTTPS  Customer information, like credit card numbers and other sensitive information, is encrypted and cannot be intercepted.  Visitors can verify you are a registered business and that you own the domain.  Customers know they are not suppose to visit sites without HTTPS, and therefore, they are more likely to trust and complete purchases from sites that use HTTPS. 4/27/2025 6 Web Search – Client Server HTML HTML 2.0 -1995; HTML 3.0 1997; HTML 4.0 1997 HTML 5.0 2014; XHTML vs. XML Server Side Scripting A number of server side scripting available. Client Side Scripting Generally UI and interaction with local machine, mostly Java Script Cascading Style Sheet (CSS) CSS is a language that describes the style of an HTML document. Web Search – Client Server HTTP Injection HTML Injection also known as Cross Site Scripting.
Java Script Cascading Style Sheet (CSS) CSS is a language that describes the style of an HTML document. Web Search – Client Server HTTP Injection HTML Injection also known as Cross Site Scripting. It is a security vulnerability that allows an attacker to inject HTML code into web pages that are viewed by other users. HTTP Response Splitting Web Application Vulnerability Web Cache poisoning Cross-User Defacement HTTP Cross Site Scripting Session Fixation 4/27/2025 7 Client-Side Vs. Server Side Scripting Web Information Discovery Directories Taxonomies populated with web pages in categories, such as Yahoo! The user to browse through a hierarchical tree of category labels. Search Engines Full-text index search engines such as Altavista, Excite and Infoseek The user with a keyword search interface supported by inverted indexes and ranking mechanisms.
category labels. Search Engines Full-text index search engines such as Altavista, Excite and Infoseek The user with a keyword search interface supported by inverted indexes and ranking mechanisms. 4/27/2025 8 Web Information Discovery Directories Vs. Search Engines A directory allows you to explore and get what you want eventually. Use a directory to find cooking-related websites. Use a directory to find travel guides in a country. A search engine brings you to the exact page on the words or phrases you are looking for. Use a search engine to find a specific recipe, by providing the name of the ingredients.
uides in a country. A search engine brings you to the exact page on the words or phrases you are looking for. Use a search engine to find a specific recipe, by providing the name of the ingredients. Use a search engine to find the transport trains schedule in Germany 4/27/2025 9 Web Search The Web Ad indexes Web spider Indexer Indexes Search User Web Characteristics Web User Interaction Web as a Graph Web Spam 4/27/2025 10 Web Characteristics Web as a Graph There are three major categories of web pages that are IN, OUT and SCC Web Economic Model Advertisement Model for Revenue Unit of Measurement CPM, CPC, CPI, CPD, CPP Complex Advertisement Models AdWords Ads Search terms Daily budget 4/27/2025 11 AdWords User Needs  Need [Brod02, RL04] Informational – want to learn about something (~40% / 65%) Navigational – want to go to that page (~25% / 15%) Transactional – want to do something (web-mediated) (~35% / 20%)  Access a service  Downloads  Shop Gray areas  Find a good hub  Exploratory search “see what’s there” Low hemoglobin United Airlines Seattle weather Mars surface images Canon S410 Car rental Brazil Sec.
a service  Downloads  Shop Gray areas  Find a good hub  Exploratory search “see what’s there” Low hemoglobin United Airlines Seattle weather Mars surface images Canon S410 Car rental Brazil Sec. 19.4.1 4/27/2025 12 How far do people look for results? (Source: iprospect.com WhitePaper_2006_SearchEngineUserBehavior.pdf) User Experience User Queries 3-4 Keywords Seldom uses syntax operators (Free Text Queries) Search Engines: Google identified two principles that helped it grow at the expense of its competitors Relevance Simple Interface Which Search engine is Bigger? 4/27/2025 13 Index Size & Estimate Capture / Recapture Method Suppose that we could pick a random page from the index of E1 and test whether it is in E2’s index and symmetrically, test whether a random page from E2 is in E1. These experiments give us fractions x and y such that our estimate is that a fraction x of the pages in E1 are in E2, while a fraction y of the pages in E2 are in E1.
a random page from E2 is in E1. These experiments give us fractions x and y such that our estimate is that a fraction x of the pages in E1 are in E2, while a fraction y of the pages in E2 are in E1. Then, letting |Ei| denote the size of the index of search engine Ei, we have x|E1| ≈ y|E2|,from which we have the form we will use |E1|/|E2| ≈ y/x Index Size & Estimate Sampling Methods Random Searches Random IP addresses Random Walks Random Queries Actual Estimate is quite challenging 4/27/2025 14 Duplicate / Near Duplicate Detection Web pages are mirrored for redundancy and high availability, hence while indexing for web search engine we may come up for duplicate (identical copy). Checksum is a common method to detect a duplicate. Near Duplicate – not identical, but a portion is common, based on pre-set threshold we can filter out the near duplicates.
plicate (identical copy). Checksum is a common method to detect a duplicate. Near Duplicate – not identical, but a portion is common, based on pre-set threshold we can filter out the near duplicates. Shingling - Given a positive integer k and a sequence of terms in a document d, define the k- shingles of d to be the set of all consecutive sequences of k terms in d. Shingling To find a near duplicate, a shingling approach is used. If there are many common shingling for some k in a pair of documents, its contents will be the same. Consider a sentence below a rose is a rose is a rose. Its shingling set Z = {a rose is a ; rose is a rose ; is a rose is ; a rose is a ; rose is a rose } , which has |Z|=5 Overlap, by Jaccard = 2/5 4/27/2025 15 Near-Duplicate Scaled Approach A pair-wise approach seems unavoidable for using shingling overlap to detect near duplicate. We can perform better, by using a large integer Hash Function and doing Hashing for shingling patterns.
h A pair-wise approach seems unavoidable for using shingling overlap to detect near duplicate. We can perform better, by using a large integer Hash Function and doing Hashing for shingling patterns. Near-Duplicate Scaled Approach 4/27/2025 16 Web Crawler Web crawling is the process by which we gather pages from the Web to index them and support a search engine. The objective of crawling is to quickly and efficiently gather as many useful web pages as possible, together with the link structure that interconnects them. web crawler is sometimes referred to as a spider. Feature a Crawler MUST provide Robustness: The crawler must be robust to deal with a large number of linked pages from a website. Sometime server traps a crawler, the crawler must identify these traps. Politeness: Web servers have both implicit and explicit policies regulating the rate at which a crawler can visit them. These politeness policies must be respected.
crawler must identify these traps. Politeness: Web servers have both implicit and explicit policies regulating the rate at which a crawler can visit them. These politeness policies must be respected. 4/27/2025 17 Feature a Crawler Should provide Distributed: The crawler should have the ability to execute in a distributed fashion across multiple machines. Scalable: The crawler architecture should permit scaling up the crawl rate by adding extra machines and bandwidth. Performance and efficiency: The crawl system should make efficient use of various system resources including processor, storage, and network bandwidth. Feature a Crawler Should provide Quality: Given that a significant fraction of all web pages are of poor utility for serving user query needs, the crawler should be biased toward fetching “useful” pages first. Freshness: In many applications, the crawler should operate in continuous mode: It should obtain fresh copies of previously fetched pages.
awler should be biased toward fetching “useful” pages first. Freshness: In many applications, the crawler should operate in continuous mode: It should obtain fresh copies of previously fetched pages. 4/27/2025 18 Feature a Crawler Should provide Extensible: Crawlers should be designed to be extensible in many ways – to cope with new data formats, new fetch protocols, and so on. This demands that the crawler architecture be modular. Architecture of a Crawler 4/27/2025 19 Architecture of a Crawler URL Frontier: containing URLs yet to be fetches in the current crawl. At first, a seed set is stored in URL Frontier, and a crawler begins by taking a URL from the seed set. DNS: domain name service resolution. Look up IP address for domain names. Fetch: generally use the http protocol to fetch the URL. Parse: the page is parsed. Texts (images, videos, and etc.) and Links are extracted.
service resolution. Look up IP address for domain names. Fetch: generally use the http protocol to fetch the URL. Parse: the page is parsed. Texts (images, videos, and etc.) and Links are extracted. URL frontier 4/27/2025 20 Typical Crawler Architecture of a Crawler Distributed Indexes By term (global Indexes) By document (Local Indexes) Connectivity Server URL are transformed into Integers values In-Link and Out-Link states are maintained. Ordering of URL based on Host, lexicographic ordering, etc
4/27/2025 1 CS4051 Information Retrieval Week 13 Muhammad Rafi April 22, 2025 Link Analysis Chapter No. 21 4/27/2025 2 Today’s Agenda Link Analysis Web as a Graph Page Rank & Markov Chain Page Rank Computation HITS Algorithm HITS vs. PageRank Conclusion Link Analysis The analysis of hyperlinks and the graph structure of the Web has been instrumental in the development of web search. In this chapter we focus on the use of hyperlinks for ranking web search results. Such link analysis is one of many factors considered by web search engines in computing a composite score for a web page on any given query. Web Page – <In-Link> and <Out-Link> <a href="http://www.acm.org/jacm/">Journal of the ACM.</a> Anchor Text Extended Anchor Text 4/27/2025 3 The Web as a graph Web pages are connected with in- and out- links Our study of link analysis builds on two intuitions: The anchor text pointing to page B is a good description of page B.
27/2025 3 The Web as a graph Web pages are connected with in- and out- links Our study of link analysis builds on two intuitions: The anchor text pointing to page B is a good description of page B. The hyperlink from A to B represents an endorsement of page B, by the creator of page A. The Web as a graph Assumption 1 : A hyperlink between pages denotes a conferral of authority (quality signal) Assumption 2: The text in the anchor of the hyperlink describe the target page( Context/ Textual description of a page). The Web is full of instances where the page B does not provide an accurate description of itself. For example, at the time of the writing of this book the home page of the IBM corporation (http://www.ibm.com)did not contain the term computer anywhere in its HTML code, despite the fact that IBM is widely viewed as the world’s largest computer maker. Thus, there is often a gap between the terms in a web page, and how web users would describe that web page.
TML code, despite the fact that IBM is widely viewed as the world’s largest computer maker. Thus, there is often a gap between the terms in a web page, and how web users would describe that web page. 4/27/2025 4 Page Rank Our first technique for link analysis assigns to every node in the web graph a numerical score between 0 and 1, known as its PageRank. The PageRank of a node will depend on the link structure of the web graph. Given a query, a web search engine computes a composite score for each web page that combines hundreds of features such as cosine similarity and term proximity, together with the PageRank score. Page Rank Consider a random surfer who begins at a web page (a node of the web graph) and executes a random walk on the Web as follows. At each time step, the surfer proceeds from his current page A to a randomly chosen web page that A hyperlinks to.
web page (a node of the web graph) and executes a random walk on the Web as follows. At each time step, the surfer proceeds from his current page A to a randomly chosen web page that A hyperlinks to. As the surfer proceeds in this random walk from node to node, he visits some nodes more often than others; intuitively, these are nodes with many links coming in from other frequently visited nodes. In the teleport operation the surfer jumps from a node to any other node in the web graph. This could happen because he types an address into the URL bar of his browser. Teleporting is uniformly performed. 4/27/2025 5 Teleporting In the teleport operation the surfer jumps from a node to any other node in the web graph. In assigning a PageRank score to each node of the web graph, we use the teleport operation in two ways: When at a node with no out-links, the surfer invokes the teleport operation.
in the web graph. In assigning a PageRank score to each node of the web graph, we use the teleport operation in two ways: When at a node with no out-links, the surfer invokes the teleport operation. At any node that has outgoing links, the surfer invokes the teleport operation with probability 0 < α < 1 and the standard random walk (follow an out-link chosen uniformly at random with probability 1 − α, where α is a fixed parameter chosen in advance. Typically, α might be 0.1. Markov Chain A Markov chain is a discrete-time stochastic process: a process that occurs in a series of time-steps in each of which a random choice is made. A Markov chain consists of N states. Each web page will correspond to a state in the Markov chain we will formulate. A Markov chain is characterized by an N × N transition probability matrix P each of whose entries is in the interval [0, 1]; the entries in each row of P add up to 1.
Markov chain we will formulate. A Markov chain is characterized by an N × N transition probability matrix P each of whose entries is in the interval [0, 1]; the entries in each row of P add up to 1. 4/27/2025 6 Markov Chain The Markov chain can be in one of the N states at any given timestep; then, the entry Pij tells us the probability that the state at the next timestep is j, conditioned on the current state being i. Each entry Pij is known as a transition probability and depends only on the current state i; this is known as the Markov property. Markov Chain as Stochastic Matrix A matrix with non-negative entries that satisfies is known as a stochastic matrix. A key property of a stochastic matrix is that it has a principal left eigenvector corresponding to its largest eigenvalue, which is 1.
negative entries that satisfies is known as a stochastic matrix. A key property of a stochastic matrix is that it has a principal left eigenvector corresponding to its largest eigenvalue, which is 1. An N-dimensional probability vector each of whose components corresponds to one of the N states of a Markov chain can be viewed as a probability distribution over its states Markov Properties 4/27/2025 7 Markov Chain Probability Matrix How to get the probability matrix? If a row of A has no 1’s, then replace each element by 1/N. For all other rows proceed as follows: Divide each 1 in A by the number of 1’s in its row. Thus, if there is a row with three m’s, then each of them is replaced by 1/m. Multiply the resulting matrix by 1 − α.
her rows proceed as follows: Divide each 1 in A by the number of 1’s in its row. Thus, if there is a row with three m’s, then each of them is replaced by 1/m. Multiply the resulting matrix by 1 − α. Add α/N to every entry of the resulting matrix, to obtain the required matrix P. Ergodic Markov chain A Markov chain is said to be ergodic if there exists a positive integer T0 such that for all pairs of states i, j in the Markov chain, if it is started at time 0 in state i then for all t > T0, the probability of being in state j at time t is greater than 0. The random walk with teleporting results in a unique distribution of steady-state probabilities over the states of the induced Markov chain. This steady-state probability for a state is the PageRank of the corresponding web page.
ng results in a unique distribution of steady-state probabilities over the states of the induced Markov chain. This steady-state probability for a state is the PageRank of the corresponding web page. 4/27/2025 8 The PageRank computation The N entries in the principal eigenvector ~π are the steady-state probabilities of the random walk with teleporting, and thus the PageRank values for the corresponding web pages. The PageRank computation 4/27/2025 9 A small Web Example Consider a web graph with three nodes 1, 2 and 3. The links are as follows: 1 → 2, 3 → 2, 2 → 1, 2 → 3. Write down the transition probability matrices for the surfer’s walk with teleporting, for the following three values of the teleport probability: (a) α = 0; (b) α = 0.5 and (c) α = 1. Advantage/Disadvantage PageRank 4/27/2025 10 HITS Hyperlink-Induced Topic Search (HITS; also known as hubs and authorities) is a link analysis algorithm that rates Web pages, developed by Jon Kleinberg.
antage/Disadvantage PageRank 4/27/2025 10 HITS Hyperlink-Induced Topic Search (HITS; also known as hubs and authorities) is a link analysis algorithm that rates Web pages, developed by Jon Kleinberg. Hubs, served as large directories that were not actually authoritative in the information that it held, but were used as compilations of a broad catalog of information that led users directly to other authoritative pages. HITS In other words, a good hub represented a page that pointed to many other pages, and a good authority represented a page that was linked by many different hubs The algorithm assigns two scores for each page: its authority, which estimates the value of the content of the page, and its hub value, which estimates the value of its links to other pages.
ent hubs The algorithm assigns two scores for each page: its authority, which estimates the value of the content of the page, and its hub value, which estimates the value of its links to other pages. 4/27/2025 11 Example HITS Algorithm 4/27/2025 12 HITS Algorithm HITS Algorithm 4/27/2025 13 HITS Algorithm An Example (HITS) 4/27/2025 14 HITS-Issues  It is query dependent, that is, the (Hubs and Authority) scores resulting from the link analysis are influenced by the search terms;  As a corollary, it is executed at query time, not at indexing time, with the associated hit on performance that accompanies query-time processing.  It is not commonly used by search engines. (Though a similar algorithm was said to be used by Teoma, which was acquired by Ask Jeeves/Ask.com.)
on performance that accompanies query-time processing.  It is not commonly used by search engines. (Though a similar algorithm was said to be used by Teoma, which was acquired by Ask Jeeves/Ask.com.)  It computes two scores per document, hub and authority, as opposed to a single score;  It is processed on a small subset of ‘relevant’ documents (a 'focused subgraph' or base set), not all documents as was the case with PageRank. Example: A subset of graph with selected Hub & Authority status. This is a result of resultant search result on “q” 4/27/2025 15 Adjacency Matrix Iterative calculation of Hub & Authority 4/27/2025 16 Iterative calculation of Hub & Authority Normalized 4/27/2025 17 HITS vs. PageRank
